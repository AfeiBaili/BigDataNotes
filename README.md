# å¤§æ•°æ®å…¨å¥—ç¬”è®°

# ç¯å¢ƒæ­å»º

> éƒ¨åˆ†ç”¨åˆ°çš„è½¯ä»¶éƒ½åœ¨å½“å‰ç¬”è®°çš„releaseçš„**software**åŒ…ä¸­

## Linuxå‰æé…ç½®

> å¦‚æœä½¿ç”¨çš„æ˜¯è™šæ‹Ÿæœº,éœ€è¦é…ç½®è™šæ‹Ÿç½‘ç»œç¯å¢ƒ,å»ºè®®ä½¿ç”¨vmè™šæ‹Ÿå¹¶ä¸”ç½‘ç»œä½¿ç”¨NATæ¨¡å¼

1. æŸ¥çœ‹è™šæ‹Ÿç½‘ç»œé…ç½®åœ¨**ç¼–è¾‘ -> è™šæ‹Ÿç½‘ç»œç¼–è¾‘å™¨**
2. æ‰“å¼€ä¹‹åç‚¹å‡»æ›´æ”¹è®¾ç½®  
   ![img.png](image/1/img.png)
3. ç‚¹å‡»NATç±»å‹,é€‰æ‹©NATè®¾ç½®  
   ![img.png](image/1/img_1.png)

é‡Œé¢åŒ…å«ç½‘å…³å’Œç½‘æ®µä¿¡æ¯,æˆ‘ä»¬éœ€è¦ç”¨åœ¨Linuxé…ç½®ä¸­

ä½¿ç”¨å…‹éš†å¹¶å…‹éš†å‡ºä¸€å°ä¸»æœºä¸¤å°å‰¯æœº,åˆ†åˆ«åä¸º**masterã€slave1ã€slave2**

### é™æ€IPé…ç½®

> æ­¤æ“ä½œéœ€è¦ä½¿ç”¨rootæƒé™ä½¿ç”¨suå‘½ä»¤åˆ‡æ¢ç”¨æˆ·`su root`

```shell
# ä½¿ç”¨vimç¼–è¾‘å™¨è¿›è¡Œæ›´æ”¹å†…å®¹
vim /etc/sysconfig/network-scripts/ifcfg-ens33
```

ä¸‹æ–¹æ˜¯æºé…ç½®æ–‡ä»¶

```
TYPE="Ethernet"
PROXY_METHOD="none"
BROWSER_ONLY="no"
BOOTPROTO="dhcp"
DEFROUTE="yes"
IPV4_FAILURE_FATAL="no"
IPV6INIT="yes"
IPV6_AUTOCONF="yes"
IPV6_DEFROUTE="yes"
IPV6_FAILURE_FATAL="no"
IPV6_ADDR_GEN_MODE="stable-privacy"
NAME="ens33"
UUID="d9e82b4b-d7c0-4ad4-a44e-1b1fbfa9c598"
DEVICE="ens33"
ONBOOT="yes"
```

æ·»åŠ æˆ–ä¿®æ”¹çš„å€¼

```
BOOTPROTO="static"      #ä¿®æ”¹ä¸ºé™æ€çš„
IPADDR="192.168.1.102"  #æ·»åŠ é™æ€IP
NETMASK="255.255.255.0" #æ·»åŠ å­ç½‘æ©ç 
GATEWAY="192.168.1.2"   #æ·»åŠ ç½‘å…³
DNS1="192.168.1.2"      #é…ç½®DNS
```

æ·»åŠ é…ç½®å®Œä¹‹åä½¿ç”¨`systemctl restart network`å‘½ä»¤é‡å¯ç½‘ç»œ

å¹¶å¯¹å…¶ä»–èŠ‚ç‚¹(ä¸»æœºæˆ–è€…å‰¯æœº)è¿›è¡Œæ·»åŠ é…ç½®,æ³¨æ„IPADDRçš„å€¼ä¸èƒ½è®¾ç½®ä¸€æ ·

### hostså’Œhostnameé…ç½®

é…ç½®å¥½ipä¹‹åå¯ä»¥ä½¿ç”¨sshè¿æ¥ç»ˆç«¯è¿›è¡Œæ›´æ–¹ä¾¿å¿«é€Ÿçš„é…ç½®äº†,ä¸‹å›¾è¿æ¥masterèŠ‚ç‚¹  
![img_2.png](image/1/img_2.png)

æ‰“å¼€ç»ˆç«¯è¾“å…¥`vim /etc/hosts`ç¼–è¾‘å‘½ä»¤è¿›è¡Œhostsæ–‡ä»¶è¿›è¡Œç¼–è¾‘

åœ¨æ–‡ä»¶ä¸­æ·»åŠ ä»¥ä¸‹å†…å®¹

```
192.168.1.102 master
192.168.1.103 slave1
192.168.1.104 slave2
```

![img_4.png](image/1/img_4.png)

é…ç½®å®Œä¸»æœºçš„hostsä¹‹åå†é…ç½®ä¸€ä¸‹ä¸»æœºåç§°

ä½¿ç”¨`vim /etc/hostname`ç¼–è¾‘å‘½ä»¤å°†é‡Œé¢çš„å†…å®¹ä¿®æ”¹æˆ

```
master
```

![img_3.png](image/1/img_3.png)

> ä¿®æ”¹äº†ä¸»æœºåä¹‹åéœ€è¦é‡å¯èŠ‚ç‚¹æ‰å¯ä»¥æ˜¾ç¤ºæ›´æ”¹åçš„åå­—

é…ç½®å®Œä¹‹åå†é…ç½®å…¶ä»–çš„èŠ‚ç‚¹hostsçš„å†…å®¹ä¸€è‡´,ä½†æ˜¯hostnameå†…å®¹åˆ†åˆ«æ”¹æˆslave1ã€slave2

ç”±äºæˆ‘ä»¬ä½¿ç”¨çš„Windowsè¿æ¥çš„èŠ‚ç‚¹,æœ€å¥½ä¹Ÿç»™Windowsçš„hostsæ–‡ä»¶é…ç½®ä¸€ä¸‹

åœ¨`C:\Windows\System32\drivers\etc\hosts`è·¯å¾„ä¸‹

> hostsæ–‡ä»¶çš„ä½œç”¨æ˜¯å½“ä½ è§£æåŸŸåçš„æ—¶å€™,ä¼šå…ˆå¯»æ‰¾æœ¬ç”µè„‘ä¸Šçš„hostsæ–‡ä»¶æœ‰æ²¡æœ‰å¯¹åº”ipå¦‚æœæœ‰å°±è¿”å›,åŸŸååŠ«æŒæ”»å‡»å°±æ˜¯åˆ©ç”¨äº†hostsæ–‡ä»¶

### é…ç½®sshå…å¯†ç™»å½•å’Œå…³é—­é˜²æŠ¤å¢™

> åœ¨hadoopä¸­å¦‚æœä¸sshè®¾ç½®å…å¯†ç™»å½•å’Œå…³é—­é˜²ç«å¢™,hadoopå°±ä¸èƒ½é€šä¿¡

åœ¨Linuxä¸­é˜²ç«å¢™æœåŠ¡çš„åç§°å«**firewalld**,ä½¿ç”¨`systemctl status firewalld`æŸ¥çœ‹æœåŠ¡çš„çŠ¶æ€

![img_5.png](image/1/img_5.png)

```shell
# å…³é—­é˜²ç«å¢™
systemctl stop firewlld
# è®¾ç½®å¼€æœºä¸è‡ªå¯
systemctl disable firewlld
```

ä½¿ç”¨`ssh-keygen -t rsa`å‘½ä»¤ç”Ÿæˆå¯†é’¥,ç”Ÿæˆæ—¶éœ€è¦ç‚¹å›è½¦é”®è¿›è¡Œç¡®è®¤,éœ€è¦åœ¨æ¯ä¸ªèŠ‚ç‚¹éƒ½æ‰§è¡Œ

æ‰§è¡Œå®Œå¯†é’¥ç”Ÿæˆä¹‹åå°±æ˜¯æ‰§è¡Œå¯†é’¥åˆ†å‘äº†,ä½¿ç”¨`ssh-copy-id åœ°å€`å‘½ä»¤åœ¨æ¯å°èŠ‚ç‚¹ä¸Šåˆ†å‘åˆ°æ¯ä¸ªèŠ‚ç‚¹

```shell
# æ‰€æœ‰ä¸»æœº
ssh-copy-id master
ssh-copy-id slave1
ssh-copy-id slave2
```

> åˆ†å‘å®Œä¹‹å,åªæ˜¯åˆ†å‘å½“å‰ç”¨æˆ·çš„,å¦‚æœä½ åªåœ¨å½“å‰ç”¨æˆ·åˆ†å‘,ä¹Ÿåªè¦å½“å‰ç”¨æˆ·æ˜¯å…å¯†ç™»å½•,å¦‚æœéœ€è¦å¯ä»¥è¿›è¡Œå¤šä¸ªç”¨æˆ·åˆ†å‘

### ç¼–å†™åˆ†å‘è„šæœ¬å’Œå‘½ä»¤æ‰§è¡Œè„šæœ¬

åˆ†å‘è„šæœ¬ç”¨äºå‘å…¶ä»–èŠ‚ç‚¹åˆ†å‘æ–‡ä»¶,ä½¿ç”¨shellç¼–ç¨‹

```shell
#!/bin/bash

if [ $# -eq 0 ]; then
        echo "æ²¡æœ‰æ–‡ä»¶"
        exit
fi
for i in master slave1 slave2; do
        if [ $HOSTNAME != $i ]; then
                for j in $@; do
                        filepath=$(realpath $j)
                        echo "æ­£åœ¨ä¸Šä¼ ${filepath}åˆ°$i"
                        rsync $filepath $USER@$i:$filepath
                done
        fi
done
```

å‘½ä»¤æ‰§è¡Œè„šæœ¬ç”¨äºå¤šèŠ‚ç‚¹æ‰§è¡Œç›¸åŒçš„å‘½ä»¤

```shell
#!/bin/bash

if [ $# -eq 0 ]; then
        echo "æ— å‘½ä»¤"
        exit
fi

for i in master slave1 slave2; do
        echo "å½“å‰${i}æ­£åœ¨æ‰§è¡Œ${*}"
        ssh $USER@$i $*
done
```

## JavaJdkç¯å¢ƒæ­å»º

å°†javaçš„è½¯ä»¶åŒ…è§£å‹åˆ°`/opt/module`è·¯å¾„ä¸‹

```shell
tar -zxvf jdk-8u202-linux-x64.tar.gz -C /opt/module
```

ä¹Ÿå¯ä»¥ä½¿ç”¨å‘½ä»¤æ‰§è¡Œè„šæœ¬æ‰¹é‡æ‰§è¡Œ

```shell
/commandsync.sh tar -zxvf /opt/software/jdk-8u202-linux-x64.tar.gz -C /opt/module/
```

> æœ€å¥½ä½¿ç”¨Java8çš„ç‰ˆæœ¬,å› ä¸ºè®¸å¤šæ¡†æ¶éƒ½æ˜¯åŸºäºJava8å¦‚æœæ›´æ¢ç‰ˆæœ¬å¯ä»¥å°†æ— æ³•å¯åŠ¨æŸäº›æ¡†æ¶

é…ç½®å®Œä¹‹åéœ€è¦é…ç½®JAVA_HOMEæ¥æŒ‡å®šJavaçš„ç¯å¢ƒå˜é‡

```shell
#åœ¨/etc/profileæ–‡ä»¶ä¸‹é…ç½®ï¼Œæˆ–è€…åœ¨/etc/profile.d/ç›®å½•ä¸‹æ–°å»ºä¸€ä¸ª.shç»“å°¾çš„è„šæœ¬
vim /etc/profile
```

åœ¨æ–‡ä»¶ä¸­å†™å…¥Javaçš„è·¯å¾„

```
JAVA_HOME=/opt/module/jdk1.8.0_202
PATH=$PATH:$JAVA_HOME/bin
```

å†™å…¥å®Œä¹‹åéœ€è¦ç”¨` . /etc/profile`å‘½ä»¤åˆ·æ–°ä¸‹è„šæœ¬,å¯ä»¥ä½¿ç”¨åˆ†å‘è„šæœ¬åˆ†å‘/etc/profileæ–‡ä»¶åˆ°æ¯ä¸ªèŠ‚ç‚¹ä¸­

> .ç‚¹æ˜¯è„šæœ¬æ‰§è¡Œçš„ä¸€ä¸ªå‘½ä»¤

## Hadoopç¯å¢ƒæ­å»º

ä½¿ç”¨å‘½ä»¤æ‰§è¡Œè„šæœ¬æ‰§è¡Œ`/commandsync.sh tar -zxvf /opt/software/hadoop-3.3.1.tar.gz -C /opt/module/`
å‘½ä»¤è§£å‹hadoopåˆ°/opt/moduleæ–‡ä»¶å¤¹

~~è·å–hadoopçš„è·¯å¾„ä½ç½®åœ¨/etc/profileä¸­æ·»åŠ HADOOP_HOMEç¯å¢ƒå˜é‡~~

> ç»æµ‹è¯• é€šè¿‡profileçš„å…¨å±€ç¯å¢ƒå˜é‡ä¼šè®©sshç”¨æˆ·æ— æ³•ä½¿ç”¨jps,å¯æ— è§†è¿™æ®µè¯

ä½¿ç”¨`vim ~/.bashrc`å‘½ä»¤æ·»åŠ ç¯å¢ƒå˜é‡

```
# MyPath
export JAVA_HOME=/opt/module/jdk1.8.0_202
export PATH=$PATH:$JAVA_HOME/bin
export HADOOP_HOME=/opt/module/hadoop-3.3.1
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
```

~~ç”¨` . /etc/profile`å‘½ä»¤åˆ·æ–°ä¸‹è„šæœ¬~~

ç”¨` . ~/.bashrc`å‘½ä»¤åˆ·æ–°ä¸‹è„šæœ¬

é…ç½®hadoop,åˆ†åˆ«é…ç½®å…­ä¸ªæ–‡ä»¶

> xmlé…ç½®,æˆ‘æ¨ä½ 

1. core-site.xml
   ```xml
   <configuration>
        <property>
                <!--é…ç½®hdfsç«¯å£ä¿¡æ¯-->
                <name>fs.defaultFS</name>
                <value>hdfs://master:8020</value>
        </property>
        <property>
                <!--è®¾ç½®hadoopæ–‡ä»¶è·¯å¾„-->
                <name>hadoop.tmp.dir</name>
                <value>/opt/module/hadoop-3.3.1/data</value>
        </property>
   </configuration>
   ```
2. hdfs-site.xml
   ```xml
      <configuration>
        <property>
                <!--é…ç½®NameNodeåœ°å€-->
                <name>dfs.namenode.http-address</name>
                <value>master:9870</value>
        </property>
        <property>
                <!--é…ç½®SecondaryNodeåœ°å€-->
                <name>dfs.namenode.secondary.http-address</name>
                <!--å¦‚æœé…ç½®åˆ°masterå¯èƒ½æ— æ³•å¯åŠ¨ï¼Œå› ä¸ºNameNodeä¹Ÿåœ¨Master-->
                <value>slave1:9868</value>
        </property>
      </configuration>
   ```
3. yarn-site.xml
   ```xml
      <configuration>
        <property>
                <!--é…ç½®ResourceManageråœ°å€-->
                <name>yarn.resourcemanager.hostname</name>
                <value>master</value>
        </property>
        <property>
                <!--é…ç½®ResourceManagerç½‘ç»œåœ°å€-->
                 <name>yarn.resourcemanager.webapp.address</name>
                <value>master:8088</value>
        </property>
        <property>
                <name>yarn.nodemanager.aux-services</name>
                <value>mapreduce_shuffle</value>
        </property>
      </configuration>
   ```
4. mapred-site.xml
   ```xml
   <configuration>
        <property>
                <!--é…ç½®mapreduceä¸ºyarn-->
                <name>mapreduce.framework.name</name>
                <value>yarn</value>
        </property>
   </configuration>
   ```
5. workers

   åœ¨workersæ–‡ä»¶ä¸­é…ç½®èŠ‚ç‚¹åœ°å€
   ```
   master
   slave1
   slave2
   ```
6. hadoop-env.sh

   æŒ‰éœ€æ±‚é…ç½®å¦‚æœå¯åŠ¨æŠ¥é”™ç¼ºå°‘ä»€ä¹ˆä»€ä¹ˆå°±æ¥è¿™ä¸ªæ–‡ä»¶é…ç½®

é…ç½®å®Œæ¯•ä¹‹åè¦å…ˆæ‰§è¡Œ`hdfs namenode -format`è¿›è¡Œhdfsæ•°æ®åˆå§‹åŒ–

åˆå§‹åŒ–å®Œæ¯•ä¹‹åä¾¿å¯ä»¥å¯åŠ¨äº†

å¯åŠ¨hdfs`start-dfs.sh`å¯åŠ¨yarn`start-yarn.sh`

å…³é—­hdfs`stop-dfs.sh`å…³é—­yarn`stop-yarn.sh`

### å…¶ä»–é…ç½®

åœ¨å·¥ä½œå®Œä¹‹å,å¹¶æ²¡æœ‰è¿è¡Œå†å²è®°å½•,è¿™å°±è¦é…ç½®ä¸€ä¸‹å†å²æœåŠ¡å™¨äº†

åœ¨mapred-site.xmlä¸­é…ç½®ä»¥ä¸‹å†…å®¹

```xml

<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>master:10020</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>master:19888</value>
    </property>
</configuration>
```

åˆ†å‘ä¹‹åå°±å¯ä»¥ä½¿ç”¨`mapred --daemon start historyserver`å‘½ä»¤è¿›è¡Œå¯åŠ¨äº†

ä¸ºäº†æ›´æ–¹ä¾¿çš„æŸ¥çœ‹èŠ‚ç‚¹ä¸Šçš„æ—¥å¿—åŠŸèƒ½,å»ºè®®å¼€å¯æ—¥å¿—èšé›†æœåŠ¡å™¨,åœ¨**yarn-site.xml**é…ç½®æ–‡ä»¶ä¸­é…ç½®ä»¥ä¸‹å†…å®¹

```
<configuration>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>master</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.address</name>
        <value>master:8088</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <!--å¼€å¯æ—¥å¿—èšé›†åŠŸèƒ½-->
        <name>yarn.log-aggregation-enable</name>
        <value>true</value>
    </property>
    <property>
        <!--è®¾ç½®æ—¥å¿—èšé›†åœ°å€-->
        <name>yarn.log.server.url</name>
        <value>http://master:19888/jobhistory/logs</value>
    </property>
    <property>
        <!--è®¾ç½®ä¿ç•™æ—¥å¿—ä¸ºä¸ƒå¤©-->
        <name>yarn.log-aggregation.retain-seconds</name>
        <value>604800</value>
    </property>
</configuration>
```

### ç¼–å†™å¯åŠ¨é›†ç¾¤è„šæœ¬

å¦‚æœæˆ‘ä»¬æ¯æ¬¡ä¸€ä¸ªä¸€ä¸ªçš„å¯åŠ¨ä¼šå¤ªéº»çƒ¦äº†,ä½†æ˜¯æˆ‘ä»¬ç¼–å†™ä¸€ä¸ªé›†ç¾¤å¯åŠ¨è„šæœ¬å°±å¾ˆæ–¹ä¾¿äº†,ä»¥ä¸‹æ˜¯shellç¼–ç¨‹

```shell
#!/bin/bash

if [ $# -eq 0 ]; then
    echo "ä½ æœªè¾“å…¥å‚æ•°"
    exit
fi

case $1 in
  "start")
  echo "å¯åŠ¨hdfs"
  start-dfs.sh
  echo "å¯åŠ¨yarn"
  start-yarn.sh
  echo "å¯åŠ¨historyserver"
  mapred --daemon start historyserver
  ;;
  "stop")
  echo "å…³é—­hdfs"
  stop-dfs.sh
  echo "å…³é—­yarn"
  stop-yarn.sh
  echo "å…³é—­historyserver"
  mapred --daemon stop historyserver
  ;;
  "restart")
  echo "å…³é—­hdfs"
  stop-dfs.sh
  echo "å…³é—­yarn"
  stop-yarn.sh
  echo "å…³é—­historyserver"
  mapred --daemon stop historyserver
  echo "å¯åŠ¨hdfs"
  start-dfs.sh
  echo "å¯åŠ¨yarn"
  start-yarn.sh
  echo "å¯åŠ¨historyserver"
  mapred --daemon start historyserver
  ;;
  *)
    echo "å‚æ•°ä¸æ˜¯start/stop/restart" 
  ;;
esac
```

## Hiveç¯å¢ƒæ­å»º

ä½¿ç”¨`tar -zxvf /opt/software/apache-hive-3.1.3-bin.tar.gz -C /opt/module/`è§£å‹

åœ¨ **/etc/profile**é…ç½®hiveç¯å¢ƒå˜é‡

```shell
export HIVE_HOME=/opt/module/apache-hive-3.1.3-bin
```

é…ç½®å®Œä¹‹ååœ¨hiveçš„ä¸»ç›®å½•ä¸‹çš„confæ–‡ä»¶å¤¹ä¸­æ–°å»ºä¸€ä¸ª**hive-site.xml**æ–‡ä»¶,æ·»åŠ ä»¥ä¸‹å†…å®¹,æ³¨æ„åœ¨hiveçš„libæ–‡ä»¶å¤¹ä¸­æ·»åŠ mysqlè¿æ¥å™¨,
æœ¬ç¬”è®°ä¸­çš„jarsæ–‡ä»¶å¤¹ä¸­æœ‰ä¸€ä¸ª**mysql-connector-java-8.0.25**ç‰ˆæœ¬çš„

> åœ¨/opt/module/apache-hive-3.1.3-bin/conf/è¿™é‡Œæ–°å»º

```
<configuration>
   <property>
      <!--è®¾ç½®è¿æ¥çš„mysqlåœ°å€-->
      <name>javax.jdo.option.ConnectionURL</name>
      <value>jdbc:mysql://123.56.82.129/hive</value>
   </property>
   <property>
      <!--è®¾ç½®è¿æ¥çš„é©±åŠ¨ç±»-->
      <name>javax.jdo.option.ConnectionDriverName</name>
      <value>com.mysql.cj.jdbc.Driver</value>
   </property>
   <property>
      <!--è®¾ç½®è¿æ¥çš„ç”¨æˆ·å-->
      <name>javax.jdo.option.ConnectionUserName</name>
      <value>root</value>
   </property>
   <property>
      <!--è®¾ç½®è¿æ¥çš„å¯†ç -->
      <name>javax.jdo.option.ConnectionPassword</name>
      <value>123456</value>
   </property>
   <property>
      <!--è®¾ç½®åˆ›å»ºè¡¨çš„é»˜è®¤è·¯å¾„-->
      <name>hive.metastore.warehouse.dir</name>
      <value>/hive/warehouse</value>
   </property>
</configuration>
```

é…ç½®å®Œä¹‹åè¿›è¡Œhiveæ•°æ®åº“çš„åˆå§‹åŒ–,ä½¿ç”¨`schematool -dbType mysql -initSchema -verbose`å‘½ä»¤è¿›è¡Œåˆå§‹åŒ–

åˆå§‹åŒ–ä¹‹åä¾¿å¯ä»¥ç”¨`hive`å‘½ä»¤å¯åŠ¨hive

å¯åŠ¨hiveä¹‹åå°±å°è¯•è¾“å…¥ä¸€äº›å‘½ä»¤æ¥æŸ¥çœ‹æ˜¯å¦å¯ä»¥è¿è¡Œ,åˆ›å»ºæ•°æ®åº“`create database myhive;`
,åˆ›å»ºä¸€å¼ ç®€å•çš„è¡¨`create table test(id int,name string);`å†å°è¯•æ’å…¥ä¸€æ¡æ•°æ®`insert into test value(1,'zs');`

å¦‚æœæ’å…¥æ•°æ®å¤±è´¥æ—¶å€™,å¯èƒ½æ˜¯mapreduceé…ç½®é”™è¯¯æˆ–ç‰ˆæœ¬ä¸å¯¹,åœ¨æ­¤ç¬”è®°ä¸Šæ–¹çš„**mapred-site.xml**é…ç½®å¹¶æ²¡æœ‰æ·»åŠ è¿™äº›å†…å®¹

```
<configuration>
        <property>
                <name>mapreduce.framework.name</name>
                <value>yarn</value>
        </property>
        <property>
                <name>mapreduce.jobhistory.address</name>
                <value>master:10020</value>
        </property>
        <property>
                <name>mapreduce.jobhistory.webapp.address</name>
                <value>master:19888</value>
        </property>
        <property>
                <!--é…ç½®mapredçš„ç¯å¢ƒä½ç½®-->         
                <name>yarn.app.mapreduce.am.env</name>
                <value>HADOOP_MAPRED_HOME=/opt/module/hadoop-3.3.1</value>
        </property>
        <property>
                <!--é…ç½®mapçš„è·¯å¾„-->         
                <name>mapreduce.map.env</name>
                <value>HADOOP_MAPRED_HOME=/opt/module/hadoop-3.3.1</value>
        </property>
        <property>
                <!--é…ç½®reduceçš„è·¯å¾„-->         
                <name>mapreduce.reduce.env</name>
                <value>HADOOP_MAPRED_HOME=/opt/module/hadoop-3.3.1</value>
        </property>
</configuration>
```

æ·»åŠ ä¹‹ååˆ†å‘,å†å°è¯•æ’å…¥æ•°æ®,å¦‚æœæ˜¾ç¤ºä¸‹å›¾è¿™æ ·æˆ–è€…åœ¨yarnwebä¸Šçœ‹åˆ°è¿›åº¦å®Œæˆå°±ä»£è¡¨æ‰§è¡ŒæˆåŠŸäº†

![img_6.png](image/1/img_6.png)

> åœ¨hiveä¸­mysqlåªæ˜¯å­˜å‚¨hiveçš„å…ƒæ•°æ®,çœŸæ­£çš„æ•°æ®å†…å®¹æ˜¯å­˜å‚¨åœ¨hadoopçš„hdfsä¸­

åœ¨hiveä¸­è¿˜æœ‰ä¸¤ä¸ªæœåŠ¡,ä»–ä»¬åœ¨hiveä¸­èµ·åˆ°äº†å…³é”®çš„ä½œç”¨

### **hiveserver2**æœåŠ¡

hiveserver2æœåŠ¡æä¾›äº†è¿œç¨‹è®¿é—®æ•°æ®çš„åŠŸèƒ½,å½“ä½¿ç”¨ç”¨æˆ·è®¿é—®çš„æ—¶å€™,å°±éœ€è¦ä¸€ä¸ªä»£ç†ç”¨æˆ·,æˆ‘ä»¬å¯ä»¥åœ¨hadoopä¸­çš„core-site.xmlæ–‡ä»¶ä¸­è¿›è¡Œé…ç½®

```
<configuration>
        <property>
                <name>fs.defaultFS</name>
                <value>hdfs://master:8020</value>
        </property>
        <property>
                <name>hadoop.tmp.dir</name>
                <value>/opt/module/hadoop-3.3.1/data</value>
        </property>
        <property>
                <!--é…ç½®afeiç”¨æˆ·å¯ä»¥ä½œä¸ºä»£ç†ç”¨æˆ·-->         
                <name>hadoop.proxyuser.afei.hosts</name>
                <value>*</value>
        </property>
        <property>
                <!--é…ç½®afeiç”¨æˆ·èƒ½å¤Ÿä»£ç†ä»»æ„ç»„-->         
                <name>hadoop.proxyuser.afei.groups</name>
                <value>*</value>
        </property>
        <property>
                <!--é…ç½®afeiç”¨æˆ·èƒ½å¤Ÿä»£ç†ä»»æ„ç”¨æˆ·-->         
                <name>hadoop.proxyuser.afei.users</name>
                <value>*</value>
        </property>
</configuration>
```

ä¸è¦å¿˜è®°åˆ†å‘åˆ°å…¶ä»–èŠ‚ç‚¹(â—'â—¡'â—)

é…ç½®å®Œä¹‹åè¿˜è¦å»é…ç½®hiveçš„çš„é…ç½®æ–‡ä»¶â•°(â€µâ–¡â€²)â•¯,é…ç½®hive-site.xmlæ–‡ä»¶

```
<configuration>
        <property>
                <name>javax.jdo.option.ConnectionURL</name>
                <value>jdbc:mysql://123.56.82.129/hive</value>
        </property>
        <property>
                <name>javax.jdo.option.ConnectionDriverName</name>
                <value>com.mysql.cj.jdbc.Driver</value>
        </property>
        <property>
                <name>javax.jdo.option.ConnectionUserName</name>
                <value>root</value>
        </property>
        <property>
                <name>javax.jdo.option.ConnectionPassword</name>
                <value>afeibaili233</value>
        </property>
        <property>
                <name>hive.metastore.warehouse.dir</name>
                <value>/hive/warehouse</value>
        </property>
        <property>
                <!--æŒ‡å®šhiveserver2çš„åœ°å€-->
                <name>hive.server2.thrift.bind.host</name>
                <value>master</value>
        </property>
        <property>
                <!--æŒ‡å®šhiveserver2çš„ç«¯å£-->
                <name>hive.server2.thrift.port</name>
                <value>10000</value>
        </property>
</configuration>
```

é…ç½®å®Œhiveserver2å°±å¯ä»¥ä½¿ç”¨hive/binä¸‹çš„**beeline**å‘½ä»¤æ¥æ‰“å¼€è‡ªå¸¦çš„å®¢æˆ·ç«¯,æ‰“å¼€ä¹‹åçš„æ ·å­

![img_7.png](image/1/img_7.png)

åœ¨>beelineä¸­è¾“å…¥`!connect jdbc:hive2://master:10000`å›è½¦ä¹‹åè¾“å…¥é…ç½®çš„ç”¨æˆ·åå’Œå¯†ç 

> ç”±äºæ²¡å¼€å¯å¯†ç æ ¡éªŒ,å¯ä»¥ä¸è¾“å¯†ç 
> æ³¨æ„ctrl+zä¼šæŠŠè¿›ç¨‹æš‚åœä¸åå°è€Œä¸æ˜¯åå°è¿è¡Œ,åå°è¿è¡Œhiveserver2ä½¿ç”¨`nohup bin/hiveserver2 > /dev/null &`
> å‘½ä»¤,/del/nullæ–‡ä»¶ä»£è¡¨ç©ºæ–‡ä»¶

é€šè¿‡ideaæ•°æ®åº“è¿æ¥hive

![img_8.png](image/1/img_8.png)

é€‰æ‹©hiveæ•°æ®åº“

![img_9.png](image/1/img_9.png)

é…ç½®å®Œä¸»æœºåœ°å€å’Œç”¨æˆ·åä¾¿å¯ä»¥è¿æ¥äº†

![img_10.png](image/1/img_10.png)

### **metastore**æœåŠ¡

metastoreæœåŠ¡çš„ä½œç”¨æ˜¯ä¸º Hive CLI æˆ–è€… Hiveserver2 æä¾›å…ƒæ•°æ®è®¿é—®æ¥å£,
hiveserver2 æ˜¯å®¢æˆ·ç«¯è¿æ¥çš„å…¥å£ï¼Œè€Œ metastore æ˜¯ hive æŸ¥è¯¢æ—¶çš„å…ƒæ•°æ®æä¾›è€…,
é»˜è®¤æ˜¯å¼€å¯çš„æ˜¯åµŒå…¥å¼æ¨¡å¼,å¼€å¯ç‹¬ç«‹è¿è¡Œæ¨¡å¼å¯ä»¥å‡å°‘å…ƒæ•°æ®æ•°æ®åº“çš„è®¿é—®å‹åŠ›

é…ç½®**hive-site.xml**é…ç½®æ–‡ä»¶

```
<configuration>
        <property>
                <name>javax.jdo.option.ConnectionURL</name>
                <value>jdbc:mysql://123.56.82.129/hive</value>
        </property>
        <property>
                <name>javax.jdo.option.ConnectionDriverName</name>
                <value>com.mysql.cj.jdbc.Driver</value>
        </property>
        <property>
                <name>javax.jdo.option.ConnectionUserName</name>
                <value>root</value>
        </property>
        <property>
                <name>javax.jdo.option.ConnectionPassword</name>
                <value>afeibaili233</value>
        </property>
        <property>
                <name>hive.metastore.warehouse.dir</name>
                <value>/hive/warehouse</value>
        </property>
        <property>
                <name>hive.server2.thrift.bind.host</name>
                <value>master</value>
        </property>
        <property>
                <name>hive.server2.thrift.port</name>
                <value>10000</value>
        </property>
        <property>
                <!--é…ç½®metastoreçš„é€šè®¯åœ°å€-->
                <name>hive.metastroe.uris</name>
                <value>thrift://master:9083</value>
        </property>
</configuration>
```

é…ç½®å®Œä¹‹åä¾¿å¯ä»¥å¯åŠ¨metastoreæœåŠ¡äº†`nohup /hive/bin/hive --service metastore > /dev/null &`

## Sparkç¯å¢ƒæ­å»º

Sparkæ˜¯ä¸€ä¸ªç¦»çº¿æ‰¹å¤„ç†æ¡†æ¶,ä½¿ç”¨`tar -zxvf /opt/software/spark-3.4.3-bin-hadoop3.tgz /opt/module/`å‘½ä»¤è§£å‹sparkåˆ°/opt/module/ä¸­

é…ç½®yarnçš„è¿è¡Œæ¨¡å¼,æ‰“å¼€sparkçš„confæ–‡ä»¶å¤¹,æ›´æ”¹é…ç½®æ–‡ä»¶åç§°**spark-env.sh.template -> spark-env.sh**

é…ç½®**spark-env.sh**æ–‡ä»¶,æ·»åŠ ä»¥ä¸‹å†…å®¹

```shell
# é…ç½®yarnçš„é…ç½®æ–‡ä»¶è·¯å¾„
YARN_CONF_DIR=/opt/module/hadoop-3.3.1/etc/hadoop/
```

> yarnæ¨¡å¼æ˜¯åŸºäºyarnçš„æ— é¡»å…¶ä»–é…ç½®

é…ç½®å®Œä¹‹åå¹¶åˆ†å‘é…ç½®æ–‡ä»¶,å°è¯•è¿è¡Œä»¥ä¸‹å‘½ä»¤è¿è¡Œ,æäº¤ä¸€ä¸ªè¿è¡Œä¾‹å­

```
bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn /spark/examples/jars/spark-examples_2.12-3.4.3.jar  10
```

### é…ç½®sparkå†å²æœåŠ¡å™¨

åœ¨sparkçš„confæ–‡ä»¶å¤¹ä¸­çš„ **spark-defaults.conf.templateé‡å‘½åä¸ºspark-defaults.conf**
å¹¶æ·»åŠ ä»¥ä¸‹å†…å®¹

```
# å¼€å¯sparkæ—¥å¿—æœåŠ¡
spark.eventLog.enabled           true
# é…ç½®hdfsæ—¥å¿—æ–‡ä»¶è·¯å¾„,åœ¨hdfsæ–‡ä»¶ç³»ç»Ÿä¸­å¿…é¡»åŒ…å«æ­¤æ–‡ä»¶å¤¹
spark.eventLog.dir               hdfs://master:8020/spark
# é…ç½®å†å²æœåŠ¡å™¨ä¸»æœºèŠ‚ç‚¹å’Œç«¯å£
spark.yarn.historyServer.address=hadoop102:18080
# é…ç½®ç«¯å£
spark.history.ui.port=18080
```

å†æ¬¡é…ç½®**spark-env.sh**æ–‡ä»¶,æ·»åŠ ä»¥ä¸‹å†…å®¹

```shell
YARN_CONF_DIR=/opt/module/hadoop-3.3.1/etc/hadoop/

# é…ç½®sparkå†å²æœåŠ¡é€‰é¡¹
export SPARK_HISTORY_OPTS="
-Dspark.history.ui.port=18080
-Dspark.history.fs.logDirectory=hdfs://hadoop102:8020/spark
-Dspark.history.retainedApplications=30"
```

## Flinkç¯å¢ƒæ­å»º

Flinkæ˜¯ä¸€ä¸ªå®æ—¶æµå¤„ç†æ¡†æ¶

è§£å‹æ–‡ä»¶`sudo /commandsync.sh tar -zxvf /opt/software/flink-1.17.2-bin-scala_2.12.tgz -C /opt/module/`

ä¿®æ”¹é›†ç¾¤é…ç½®,åœ¨flink/conf/ä¸­çš„**flink-conf.yaml**ä¸­ä¿®æ”¹ä»¥ä¸‹é…ç½®

```yaml
# JobManagerèŠ‚ç‚¹åœ°å€
jobmanager.rpc.address: master
jobmanager.bind-host: 0.0.0.0
rest.address: master
rest.bind-address: 0.0.0.0

# TaskManagerèŠ‚ç‚¹åœ°å€
taskmanager.bind.host: 0.0.0.0
taskmanager.host: master
```

é…ç½®**workers**æ–‡ä»¶,æ·»åŠ ä»¥ä¸‹å†…å®¹

```
master
slave1
slave2
```

é…ç½®**masters**æ–‡ä»¶,ä¿®æ”¹ä¸ºä»¥ä¸‹å†…å®¹

```
master:8081
```

ä¿®æ”¹ä¹‹åå°†é…ç½®æ–‡ä»¶åˆ†å‘ä¸‹å»,å¹¶ä¿®æ”¹**flink-conf.yaml**ä¸­çš„**taskmanager.host: master**
ä¿®æ”¹ä¸º**taskmanager.host: å½“å‰ä¸»æœºåœ°å€åç§°**

è¾“å…¥`bin/start-cluster.sh`ä»¥Standaloneæ¨¡å¼å¯åŠ¨flinké›†ç¾¤

> Standaloneæ¨¡å¼,æ—¢æ˜¯å•æœºè¿è¡Œæ¨¡å¼,ä¸ä¾èµ–å…¶ä»–èµ„æº,åœ¨çœŸæ­£è¿è¡Œä¸­,å»ºè®®ä½¿ç”¨yarnæ¨¡å¼

### yarnæ¨¡å¼éƒ¨ç½²

é…ç½®ç¯å¢ƒå˜é‡åœ¨ **/etc/profile** ä¸­é…ç½®

```shell
# å‰æé…ç½®HADOOP_HOMEå’ŒPATH,é…ç½®hadoopé…ç½®æ–‡ä»¶è·¯å¾„
export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
# é…ç½®hadoop classpath
export HADOOP_CLASSPATH=`hadoop classpath`
```

> \`hadoop classpath\` åå¼•å·ä»£è¡¨æ‰§è¡Œé‡Œé¢çš„å†…å®¹,
> ${HADOOP_HOME}/etc/hadoop ${}ä»£è¡¨è¯»å–é‡Œé¢çš„å˜é‡

åŸºäºyarnæ¨¡å¼è¿è¡Œè¦å…ˆå¯åŠ¨hadoopé›†ç¾¤

åœ¨flinkæ–‡ä»¶å¤¹ä¸­è¾“å…¥`bin/yarn-session`å¯åŠ¨yarnçš„ä¼šè¯æ¨¡å¼,å¯åŠ¨ä¹‹ååœ¨yarnä»»åŠ¡ä¸­ä¼šæœ‰ä¸€ä¸ªä»»åŠ¡æ­£åœ¨è¿›è¡Œ

![img_12.png](image/1/img_12.png)

å•ä½œä¸šæ¨¡å¼å¯ä»¥ç›´æ¥å¯åŠ¨flinké›†ç¾¤,ä½¿ç”¨`flink run`å‘½ä»¤

| å‚æ•° | æè¿°          |
|----|-------------|
| -t | æŒ‡å®šè¿è¡Œæ¨¡å¼      |
| -c | æŒ‡å®šå…¨ç±»å       |
| -d | åˆ†ç¦»æ¨¡å¼,é˜²æ­¢ç»ˆç«¯é˜»å¡ |
| -D | æŒ‡å®šyarnId    |

> åˆ†ç¦»æ¨¡å¼å¯èƒ½æŠ›å‡ºä¸€ä¸ªæ£€æŸ¥ç±»åŠ è½½å™¨çš„å¼‚å¸¸

```shell
bin/flink run -t yarn-per-job -c com.afeibaili.FlinkDemo FlinkDemo.jar
```

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹è¿è¡Œæˆ–å–æ¶ˆè¿è¡Œä½œä¸š

```shell
# æŸ¥çœ‹è¿è¡Œä»»åŠ¡
bin/flink list -t yarn-per-job -Dyarn.application.id=application_XXX_XXX
# æ ¹æ®è¿è¡Œä»»åŠ¡Idå–æ¶ˆä»»åŠ¡
bin/flink cancel -t yarn-per-job -Dyarn.application.id=application_XXX_XXX <JobID>
```

åº”ç”¨æ¨¡å¼å’Œä½†ä½œä¸šæ¨¡å¼ç±»ä¼¼,ä½†æœ‰äº›åŒºåˆ«

```shell
# ä»–ä»¬çš„åŒºåˆ« run -> run-applicationï¼Œyarn-per-job -> yarn-application
bin/flink run-application -t yarn-application -c com.afeibaili.FlinkDemo FlinkDemo.jar
```

> å•ä½œä¸šæ¨¡å¼ä¸ºæ¯ä¸ªä»»åŠ¡éƒ½å¼€å¯ä¸€ä¸ªé›†ç¾¤,å¼€é”€æ¯”è¾ƒå¤§ åº”ç”¨ç¨‹åºæ¨¡å¼åªå¼€å¯ä¸€ä¸ªé›†ç¾¤

è¿˜å¯ä»¥æŠŠåŒ…æ”¾åˆ°hdfsä¸Šè¿›è¡Œè¿è¡Œ,æŠŠflinkçš„libæ–‡ä»¶å¤¹å’Œpluginsæ”¾åˆ°hdfs,é€šè¿‡ä»¥ä¸‹å‚æ•°å‘½ä»¤è¿›è¡Œè¿è¡Œ

> libå’Œpluginsæ”¾åˆ°ä¸€ä¸ªæ–‡ä»¶å¤¹ä¸­

```shell
# å‰é¢ä¸¤ä¸ªå‚æ•°å°±ä¸å¤šè¯´äº†ï¼Œ-Dyarn.provided.lib.dirs æŒ‡å®šhdfsä¸Šåº“çš„ä½ç½®ï¼Œåé¢è·Ÿä¸Šå…¨ç±»åå’ŒjaråŒ…åœ¨hdfsä¸Šçš„ä½ç½®
bin/flink run-application -t yarn-application -Dyarn.provided.lib.dirs="hdfs://master:8020/flink" -c com.afeibaili.FlinkDemo hdfs://master:8020/flink/jar/FlinkDemo.jar
```

### flinkå†å²æœåŠ¡å™¨é…ç½®

é…ç½®**flink/conf/flink-conf.yaml**æ–‡ä»¶,ä¿®æ”¹ä»¥ä¸‹é…ç½®å‚æ•°

æ³¨æ„åˆ›å»ºhdfsæœåŠ¡å™¨ä¸Šçš„æ–‡ä»¶å¤¹

```yaml
jobmanager.archive.fs.dir: hdfs://master:8020/flink/log
historyserver.web.address: master
historyserver.web.port: 8082
historyserver.archive.fs.dir: hdfs://master:8020/flink/log
historyserver.archive.fs.refresh-interval: 10000
```

![img_13.png](image/1/img_13.png)

ä½¿ç”¨`bin/historyserver.sh start`å¯åŠ¨flinkå†å²æœåŠ¡å™¨,startæ›´æ¢stopä¸ºå…³é—­å†å²æœåŠ¡å™¨

## Kafkaç¯å¢ƒæ­å»º

kafkaæ˜¯ä¸€æ¬¾é«˜åå,é«˜å¯ç”¨æ¶ˆæ¯é˜Ÿåˆ—,kafkaæœ‰ä¸¤ç§éƒ¨ç½²æ¨¡å¼,ä¸€ç§æ˜¯**Zookeeper**,å¦ä¸€ç§æ˜¯**KRaft**éƒ¨ç½²æ¨¡å¼

### zookeeperç¯å¢ƒæ­å»º

ä½¿ç”¨å‘½ä»¤`tar -zxvf /opt/software/apache-zookeeper-3.8.1-bin.tar.gz -C /opt/module/`è§£å‹

è§£å‹ä¹‹ååˆ›å»ºæ•°æ®æ–‡ä»¶å¤¹åœ¨zookeeperæ–‡ä»¶å¤¹ä¸­

![img_14.png](image/1/img_14.png)

åœ¨åˆ›å»ºçš„æ–‡ä»¶å¤¹ä¸­æ·»åŠ ä¸€ä¸ªåä¸ºmyidçš„æ–‡ä»¶é‡Œé¢åªå†™ä¸€ä¸ª1,ä»£è¡¨èŠ‚ç‚¹1,å¯ä»¥é€šè¿‡å‘½ä»¤å¿«æ·ç”Ÿæˆ`echo "1" > /opt/module/apache-zookeeper-3.8.1-bin/data/myid`

æ‰“å¼€zookeeperçš„é…ç½®æ–‡ä»¶å¤¹,æ‰¾åˆ°**zoo_sample.cfg**å‘½åä¸º**zoo.cfg**å¹¶é…ç½®ä»¥ä¸‹å†…å®¹

```
# é…ç½®zookeeperçš„æ•°æ®è·¯å¾„
dataDir=/opt/module/apache-zookeeper-3.8.1-bin/data

# é…ç½®zookeeperçš„èŠ‚ç‚¹ç«¯å£å’Œé€‰ä¸¾ç«¯å£(é€‰ä¸¾ç®¡ç†ç”¨çš„)
server.1=master:2888:3888
server.2=slave1:2888:3888
server.3=slave2:2888:3888
```

![img_15.png](image/1/img_15.png)

ç„¶ååˆ†å‘é›†ç¾¤å¹¶ä¿®æ”¹`data/myid`æ–‡ä»¶çš„èŠ‚ç‚¹,masterä¸º1,slave1ä¿®æ”¹ä¸º2,slave2ä¿®æ”¹ä¸º3

ä¿®æ”¹å®Œä¹‹åä¾¿å¯ä»¥å¯åŠ¨zookeeperäº†,å¯åŠ¨zookeeperéœ€è¦åœ¨æ¯å°ä¸»æœºä¸Šéƒ½å¯åŠ¨,å†™ä¸€ä¸ªå¯åŠ¨è„šæœ¬ä¼šæ–¹ä¾¿è®¸å¤š

```shell
#!/bin/bash

case $1 in
"start")
        echo "zookeeperå¼€å¯"
        /commandsync.sh /opt/module/apache-zookeeper-3.8.1-bin/bin/zkServer.sh start
;;
"stop")
        echo "zookeeperå…³é—­"
        /commandsync.sh /opt/module/apache-zookeeper-3.8.1-bin/bin/zkServer.sh stop
;;
"restart")
        echo "zookeeperå…³é—­"
        /commandsync.sh /opt/module/apache-zookeeper-3.8.1-bin/bin/zkServer.sh stop
        echo "zookeeperå¼€å¯"
        /commandsync.sh /opt/module/apache-zookeeper-3.8.1-bin/bin/zkServer.sh start
;;
"status")
        echo "zookeeperçŠ¶æ€"
        /commandsync.sh /opt/module/apache-zookeeper-3.8.1-bin/bin/zkServer.sh status
;;
*)
        echo "è¯·è¾“å…¥(start|stop|restart|status)"
;;
esac
```

å¯åŠ¨å‘½ä»¤`bin/zkServer.sh start`,å…³é—­å‘½ä»¤`bin/zkServer.sh stop`,æ›´æ”¹ä¸ºstart statuså˜æˆæŸ¥çœ‹è¿è¡ŒçŠ¶æ€

### åŸºäºzookeeperçš„kafkaç¯å¢ƒæ­å»º

æ­å»ºå¥½zookeeperåè§£å‹kafkaå®‰è£…åŒ…`tar -zxvf /opt/software/kafka_2.12-3.8.1.tgz -C /opt/module/`

è§£å‹ä¹‹åå¼€å§‹é…ç½®**kafka/config/server.properties**æ–‡ä»¶,æ›´æ”¹ä»¥ä¸‹å†…å®¹

```
# æ›´æ”¹idä¸ºzookeeperé…ç½®myid,ç»Ÿä¸€èµ·æ¥
broker.id=1
# æ›´æ”¹å¯¹å¤–æš´éœ²çš„åœ°å€ç«¯å£
advertised.listeners=PLAINTEXT://master:9092
# é…ç½®æŒä¹…åŒ–è·¯å¾„
log.dirs=/opt/module/kafka_2.12-3.8.1/data
# é…ç½®zookeeperè¿æ¥
zookeeper.connect=master:2181,slave1:2181,slave2:2181
```

åˆ†å‘ä¹‹å,æ›´æ”¹å…¶ä»–èŠ‚ç‚¹çš„é…ç½®æ–‡ä»¶ä¿¡æ¯**broker.id**å’Œ**advertised.listeners**

å¯åŠ¨kafka`bin/kafka-server-start.sh --daemon config/server.properties`å‘½ä»¤å’Œå…³é—­kafka`bin/kafka-server-stop.sh`å‘½ä»¤,
åˆ†åˆ«åœ¨é›†ç¾¤å¯åŠ¨kafkaå¾ˆéº»çƒ¦,å¯ä»¥ç¼–å†™ä¸€ä¸ªå¯åŠ¨è„šæœ¬,ç”¨äºå¿«é€Ÿå¯åŠ¨kafka

```shell
#!/bin/bash

case $1 in
"start")
        echo "zookeeperå¼€å¯"
        /commandsync.sh /opt/module/apache-zookeeper-3.8.1-bin/bin/zkServer.sh start
        echo "kafkaå¼€å¯"
        /commandsync.sh /opt/module/kafka_2.12-3.8.1/bin/kafka-server-start.sh -daemon /opt/module/kafka_2.12-3.8.1/config/server.properties
;;
"stop")
        echo "kafkaå…³é—­"
        /commandsync.sh /opt/module/kafka_2.12-3.8.1/bin/kafka-server-stop.sh
        echo "zookeeperå…³é—­"
        /commandsync.sh /opt/module/apache-zookeeper-3.8.1-bin/bin/zkServer.sh stop
;;
"restart")
        echo "kafkaå…³é—­"
        /commandsync.sh /opt/module/kafka_2.12-3.8.1/bin/kafka-server-stop.sh
        echo "zookeeperå…³é—­"
        /commandsync.sh /opt/module/apache-zookeeper-3.8.1-bin/bin/zkServer.sh stop
                echo "zookeeperå¼€å¯"
        /commandsync.sh /opt/module/apache-zookeeper-3.8.1-bin/bin/zkServer.sh start
        echo "kafkaå¼€å¯"
        /commandsync.sh /opt/module/kafka_2.12-3.8.1/bin/kafka-server-start.sh -daemon /opt/module/kafka_2.12-3.8.1/config/server.propertie
;;
*)
        echo "è¯·è¾“å…¥(start|stop|restart)"
;;
esac
```

èµ°åˆ°è¿™é‡Œå°±æ˜¯é…ç½®æˆåŠŸäº†å¯ä»¥ç”¨ IDEA BigData æ’ä»¶æ¥å°è¯•è¿æ¥

## Flumeç¯å¢ƒæ­å»º

ä½¿ç”¨`wget https://dlcdn.apache.org/flume/1.11.0/apache-flume-1.11.0-bin.tar.gz `å‘½ä»¤æ¥è·å–Flume

è·å–ä¹‹åè¿›è¡Œè§£å‹`tar -zxvf apache-flume-1.11.0-bin.tar.gz -C /opt/module/`

é€šå¸¸è¿è¡Œflumeéƒ½éœ€è¦ç¼–å†™ä¸€ä¸ªç‰¹å®šçš„é…ç½®æ–‡ä»¶ç”¨æ¥è¿è¡Œæ—¶æŒ‡å®šè¿™ä¸ªé…ç½®æ–‡ä»¶,ä¸‹é¢æ˜¯ä¸€ä¸ªåŸºäºSocketæ•°æ®æºçš„æ–‡ä»¶ç¼–å†™

```
# ç¬¬ä¸€éƒ¨åˆ† å£°æ˜å˜é‡,a1æŒ‡çš„æ˜¯agentåœ¨ç¼–å†™è¿è¡Œå‘½ä»¤æ˜¯ä¼ å…¥a1
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# ç¬¬äºŒéƒ¨åˆ† æŒ‡å®šè¾“å…¥æº
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 33393

# ç¬¬ä¸‰éƒ¨åˆ† æŒ‡å®šè¾“å‡ºæº
a1.sinks.k1.type = logger

# ç¬¬å››éƒ¨åˆ† é…ç½®ç¼“å­˜
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# ç¬¬äº”éƒ¨åˆ† é…ç½®è¾“å…¥è¾“å‡ºçš„ç¼“å­˜
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

é…ç½®å¥½ä¹‹åä½¿ç”¨`bin/flume-ng agent -c conf -f job/net-flume-logger.conf -n a1`å‘½ä»¤è¿›è¡Œå¯åŠ¨,
å†å¯åŠ¨ncä½¿ç”¨`nc localhost 33393`å‘½ä»¤å¯åŠ¨

> å¦‚æœä¸é…ç½®confç›®å½•ä¸‹çš„log4j.propertiesé…ç½®**flume.root.logger=INFO,LOGFILE,console**
> ä¼šæ— æ³•æ‰“å°å‡ºæ§åˆ¶å°,åªåœ¨æ—¥å¿—ä¸­æ‰“å°

## HBaseç¯å¢ƒæ­å»º

å¯ä½¿ç”¨`wget https://dlcdn.apache.org/hbase/2.5.10/hbase-2.5.10-bin.tar.gz `å‘½ä»¤æ¥è·å–HBase

è·å–å®Œä¹‹åè¿›è¡Œè§£å‹`sudo tar -zxvf hbase-2.5.10-bin.tar.gz -C /opt/module/`

è§£å‹å®Œä¹‹åé…ç½®hbase-env.sh,æ‰¾åˆ°

```
# å°†ä¸€ä¸‹é…ç½®æ”¹æˆfalse é…ç½®æè¿°:æ˜¯å¦ä½¿ç”¨hbaseè‡ªå¸¦çš„zookeeper,æˆ‘ä»¬ç»™ä»–æ”¹æˆè‡ªå·±çš„zookeeper
export HBASE_MANAGES_ZK=false
```

ä¿®æ”¹confä¸‹çš„hbase-site.xmlæ–‡ä»¶

```
   <property>
      <!--æ˜¯å¦ä¸ºé›†ç¾¤æ¨¡å¼-->
      <name>hbase.cluster.distributed</name>
      <value>true</value>
   </property>
   <property>
      <!--zookeeperçš„èŠ‚ç‚¹åœ°å€-->
      <name>hbase.zookeeper.quorum</name>
      <value>master,slave1,slave2</value>
   </property>
   <property>
      <!--é…ç½®ä¸»æ–‡ä»¶å¤¹çš„ä½ç½®-->
      <name>hbase.rootdir</name>
      <value>hdfs://master:8020/hbase</value>
   </property>
```

é…ç½®**regionservers**æ–‡ä»¶,ç”¨æ¥é…ç½®hbaseçš„é›†ç¾¤èŠ‚ç‚¹

```
master
slave1
slave2
```

é…ç½®ä»¥ä¸Šä¿¡æ¯åå°±å¯ä»¥åˆ†å‘hbaseåˆ°å…¶ä»–èŠ‚ç‚¹äº†

> å¦‚æœè¿è¡Œæ—¶æŠ¥é”™å¯ä»¥å°è¯•åœ¨æŠ¥é”™ä¸­æ·»åŠ ä»¥ä¸‹é…ç½®

```
<property>
   <name>hbase.wal.provider</name>
   <value>filesystem</value>
</property>
```

### å…¶ä»–é…ç½®

é…ç½®é«˜å¯ç”¨

åœ¨conf/ä¸‹åˆ›å»º**backup-masters**æ–‡ä»¶,å¯ä»¥ä½¿ç”¨`echo slave1 > backup-masters`å‘½ä»¤å¿«é€Ÿå†™å…¥å†…å®¹,å†è¿›è¡Œåˆ†å‘

# è¯­è¨€å­¦ä¹ 

# æ¡†æ¶å­¦ä¹ 

## Hadoop

## MySql

## Hive

## HBase

## Flume

## Kafka

## Spark

## Flink

Apache Flink æ˜¯ä¸€ä¸ªæ¡†æ¶å’Œåˆ†å¸ƒå¼å¤„ç†å¼•æ“ï¼Œç”¨äºå¯¹æ— ç•Œå’Œæœ‰ç•Œæ•°æ®æµè¿›è¡Œæœ‰çŠ¶æ€è®¡ç®—

### çª—å£

Flinkä¸»è¦å¤„ç†æ— ç•Œæ•°æ®æµ,æ•°æ®æºæºä¸æ–­,çª—å£çš„å‡ºç°å°±æ˜¯å°†æºæºä¸æ–­çš„æ•°æ®æµåˆ‡åˆ†æˆä¸€å—ä¸€å—çš„æ•°æ®,
çª—å£åŒ…å«ä¸¤å¤§ç±»çª—å£ä¸€ç§æ˜¯è®¡æ•°çª—å£(TimeWindow),å¦ä¸€ç§æ˜¯æ—¶é—´çª—å£(CountWindow)

> å‡å¦‚æ•°æ®å°±æ˜¯ä¸€ä¸ªæºæºä¸æ–­çš„æ°´é¾™å¤´ğŸš°,çª—å£å°±æ˜¯æ°´é¾™å¤´åº•ä¸‹çš„æ°´æ¡¶ğŸª£,æ¥å®Œä¸€æ¡¶å†æ¥ä¸€æ¡¶

Flinkæœ‰å¤šç§çª—å£è§„åˆ™åˆ†åˆ«æ˜¯**æ»šåŠ¨çª—å£ã€æ»‘åŠ¨çª—å£ã€ä¼šè¯çª—å£ã€å…¨å±€çª—å£**

æ»šåŠ¨çª—å£: æ•°æ®é¦–å°¾ç›¸æ¥,ä¸åŒ…å«é‡å¤æ•°æ®
æ»‘åŠ¨çª—å£: æ•°æ®æœ‰é‡å ,åŒ…å«é‡å¤æ•°æ®
ä¼šè¯çª—å£: ä¸ä¼šé‡å ,é•¿åº¦ä¸å›ºå®š,åªèƒ½åœ¨æ—¶é—´çª—å£(TimeWindow)ä¸­å‡ºç°
å…¨å±€çª—å£: çª—å£é»˜è®¤ä¸ä¼šè§¦å‘è®¡ç®—,éœ€è¦è‡ªå®šä¹‰çª—å£è§¦å‘å™¨

### çª—å£API

åœ¨çª—å£APIä¸­æœ‰ä¸¤ç§çª—å£APIä¸€ç§æ˜¯`windowAll()`ä¸€ç§æ˜¯ç»è¿‡keyByä¹‹åçš„`keyBy(...).window()`

window: æ•°æ®æ ¹æ®keyByåˆ†åŒºè¿è¡Œ,æ¯ä¸ªkeyéƒ½å¼€ä¸€ä¸ªçª—å£ç‹¬ç«‹è¿è¡Œ
windowAll: å°†ä¼ å…¥è¿›æ¥çš„æ•°æ®è¿è¡Œåœ¨ä¸€ä¸ªåˆ†åŒºä¸Š

```java
public static void main(String[] args) {
    StreamExecutionEnvironment environment = StreamExecutionEnvironment.getExecutionEnvironment();
    ArrayList<String> strings = new ArrayList<>();
    for (int i = 0; i < 10; i++) {
        strings.add(i + "");
    }
    //ä½¿ç”¨windowAll()
    environment.fromCollection(strings).windowAll();
}
```

ä¸‹é¢æ˜¯WindowAll API

```java
    public static void main(String[] args) throws Exception {
    StreamExecutionEnvironment environment = StreamExecutionEnvironment.getExecutionEnvironment();
    ArrayList<String> strings = new ArrayList<>();
    for (int i = 0; i < 10; i++) {
        strings.add(i + "");
    }
    DataStreamSource<String> streamSource = environment.fromCollection(strings);
//        streamSource.windowAll(SlidingProcessingTimeWindows.of(Time.seconds(10), Time.seconds(2))); åŸºäºæ—¶é—´çš„æ»‘åŠ¨çª—å£
//        streamSource.windowAll(TumblingProcessingTimeWindows.of(Time.seconds(10))); åŸºäºæ—¶é—´çš„æ»šåŠ¨çª—å£
//        streamSource.windowAll(ProcessingTimeSessionWindows.withGap(Time.seconds(10))); åŸºäºæ—¶é—´çš„ä¼šè¯çª—å£

//        streamSource.countWindowAll(2); åŸºäºè®¡æ•°çš„æ»šåŠ¨çª—å£
//        streamSource.countWindowAll(2,1); åŸºäºè®¡æ•°çš„æ»‘åŠ¨çª—å£

    environment.execute();
}
```

åœ¨è½¬æ¢windowApiä¸­è°ƒç”¨è¿™äº›æ–¹æ³•`reduce()`ã€`aggregate()`ã€`apply()`ã€`process()`**å¯è½¬å›DataStream**

reduceæ–¹æ³•

```java
public static void main(String[] args) throws Exception {
    //åˆ›å»ºæ‰§è¡Œç¯å¢ƒ
    StreamExecutionEnvironment environment = StreamExecutionEnvironment.getExecutionEnvironment();
    //æ¨¡æ‹Ÿæ•°æ®æµ
    ArrayList<String> strings = new ArrayList<>();
    for (int i = 0; i < 10; i++) {
        strings.add(i + "");
    }
    //è·å–DataSteam
    DataStreamSource<String> streamSource = environment.fromCollection(strings);
    //ä½¿ç”¨è®¡æ•°çª—å£è½¬æ¢ä¸ºWindowStreamå¹¶å°†çª—å£å¤§å°è®¾ç½®ä¸º2
    AllWindowedStream<String, GlobalWindow> countWindowAll = streamSource.countWindowAll(2);
    //è¿›è¡Œreduceæ“ä½œ
    SingleOutputStreamOperator<String> reduce = countWindowAll.reduce(new ReduceFunction<String>() {
        @Override
        //value1ã€value2: ä¼ è¿›æ¥çš„å€¼ è¿”å›å€¼: è¿”å›ä¸€ä¸ªString
        public String reduce(String value1, String value2) throws Exception {
            return Integer.parseInt(value1) + Integer.parseInt(value2) + "";
        }
    });
    //æ‰“å°è¾“å‡º
    reduce.print();
    //æ‰§è¡Œ
    environment.execute();
}
```

> åœ¨å¤§éƒ¨åˆ†åœºæ™¯ä¸­ä½¿ç”¨reduceå°±å¯ä»¥åšåˆ°äº†,ä½†æ˜¯ä½¿ç”¨aggregateæ–¹æ³•å¯ä»¥æŒ‡å®šè¾“å‡ºç±»å‹

aggregateæ–¹æ³•

```java
 public static void main(String[] args) throws Exception {
    StreamExecutionEnvironment environment = StreamExecutionEnvironment.getExecutionEnvironment();
    ArrayList<String> strings = new ArrayList<>();
    for (int i = 0; i < 10; i++) {
        strings.add(i + "");
    }
    DataStreamSource<String> streamSource = environment.fromCollection(strings);
    AllWindowedStream<String, GlobalWindow> countWindowAll = streamSource.countWindowAll(5);
    //ä½¿ç”¨aggregateæ–¹æ³•ç¬¬ä¸€ä¸ªæ³›å‹æ˜¯è¾“å…¥ç±»å‹ï¼Œç¬¬äºŒä¸ªæ³›å‹æ˜¯ç´¯åŠ å™¨ç±»å‹ï¼Œç¬¬ä¸‰ä¸ªæ³›å‹æ˜¯è¾“å‡ºç±»å‹
    countWindowAll.aggregate(new AggregateFunction<String, Integer, Integer>() {
        @Override
        //åˆ›å»ºç´¯åŠ å™¨è°ƒç”¨
        public Integer createAccumulator() {
            System.out.println("åˆ›å»ºç´¯åŠ å™¨");
            return 0;
        }

        @Override
        //ä¼ è¿›æ¥çš„å€¼å°±ä¼šèµ°è¿™ä¸ªæ–¹æ³•
        public Integer add(String value, Integer accumulator) {
            System.out.println("ç›¸åŠ ");
            return Integer.parseInt(value) + accumulator;
        }

        @Override
        //è¿”å›ç»“æœæ—¶è°ƒç”¨
        public Integer getResult(Integer accumulator) {
            System.out.println("è¿”å›ç»“æœ");
            return accumulator;
        }

        @Override
        public Integer merge(Integer a, Integer b) {
            System.out.println("åªæœ‰ä¼šè¯åº•å±‚æ‰ä¼šç”¨åˆ°");
            return 0;
        }
    }).print();
    environment.execute();
}
```

å…¨çª—å£å‡½æ•°

å¦‚æœæˆ‘ä»¬çš„è¦æ±‚éœ€è¦çª—å£çš„ä¸Šä¸‹æ–‡ä¸­çš„ä¸€äº›ä¿¡æ¯,èšåˆæ–¹æ³•æ˜¯åšä¸åˆ°çš„,å°±éœ€è¦çª—å£å‡½æ•°

```java
 public static void main(String[] args) throws Exception {
    StreamExecutionEnvironment environment = StreamExecutionEnvironment.getExecutionEnvironment();
    ArrayList<String> strings = new ArrayList<>();
    for (int i = 0; i < 10; i++) {
        strings.add(i + "");
    }
    DataStreamSource<String> streamSource = environment.fromCollection(strings);
    AllWindowedStream<String, GlobalWindow> countWindowAll = streamSource.countWindowAll(5);
    //è¿‡æ—¶çš„çª—å£æ–¹æ³•
    countWindowAll.apply(new AllWindowFunction<String, Integer, GlobalWindow>() {
        @Override
        public void apply(GlobalWindow window, Iterable<String> values, Collector<Integer> out) throws Exception {
            values.forEach(value -> out.collect(Integer.parseInt(value)));
        }
    }).print();
    environment.execute();
}
```

> ä½¿ç”¨`process()`åŠŸèƒ½æ¯”applyæ›´åŠ é½å…¨

```java
 public static void main(String[] args) throws Exception {
    StreamExecutionEnvironment environment = StreamExecutionEnvironment.getExecutionEnvironment();
    ArrayList<String> strings = new ArrayList<>();
    for (int i = 0; i < 10; i++) {
        strings.add(i + "");
    }
    DataStreamSource<String> streamSource = environment.fromCollection(strings);
    AllWindowedStream<String, GlobalWindow> countWindowAll = streamSource.countWindowAll(5);
    countWindowAll.process(new ProcessAllWindowFunction<String, String, GlobalWindow>() {
        /**
         * å¦‚æœæ˜¯keyByä¹‹åçš„windowä¼šæœ‰ä¸ªkeyçš„å½¢å‚
         * @param context çª—å£çš„ä¸Šä¸‹æ–‡
         * @param elements æ•°æ®çš„é›†åˆ
         * @param out é‡‡é›†å™¨å¯¹è±¡
         */
        @Override
        public void process(ProcessAllWindowFunction<String, String, GlobalWindow>.Context context, Iterable<String> elements, Collector<String> out) throws Exception {
            int count = 0;
            for (String element : elements) {
                count += Integer.parseInt(element);
            }
            out.collect(count + ":åŒ…å«è¿™äº›æ•°æ®=>" + elements);
        }
    }).print();
    environment.execute();
}
```

èšåˆå‡½æ•°å ç”¨å†…å­˜å°,ä½†æ˜¯æ‹¿ä¸åˆ°ä¸Šä¸‹æ–‡,ä½¿ç”¨å…¨çª—å£å‡½æ•°è™½ç„¶å¯ä»¥ä½¿ç”¨ä¸Šä¸‹æ–‡,
ä½†æ˜¯å ç”¨å†…å­˜å¤§,æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å…¨çª—å£å‡½æ•°ç»“åˆèšåˆå‡½æ•°æ¥ä½¿ç”¨ä»–ä»¬çš„ä¼˜ç‚¹

```java
public static void main(String[] args) throws Exception {
    StreamExecutionEnvironment environment = StreamExecutionEnvironment.getExecutionEnvironment();
    ArrayList<String> strings = new ArrayList<>();
    for (int i = 0; i < 10; i++) {
        strings.add(i + "");
    }
    DataStreamSource<String> streamSource = environment.fromCollection(strings);
    AllWindowedStream<String, GlobalWindow> countWindowAll = streamSource.countWindowAll(5);
    //ä½¿ç”¨aggregate(new AggregateFunction(),new ProcessAllWindowFunction)æ¥å®ç°ä»–ä»¬ç”¨äºæ›´å¼ºçµæ´»çš„è¿ç®—
    countWindowAll.aggregate(new AggregateFunction<String, Integer, Integer>() {
        @Override
        public Integer createAccumulator() {
            return 0;
        }

        @Override
        public Integer add(String value, Integer accumulator) {
            return 0;
        }

        @Override
        public Integer getResult(Integer accumulator) {
            return 0;
        }

        @Override
        public Integer merge(Integer a, Integer b) {
            return 0;
        }
    }, new ProcessAllWindowFunction<Integer, Integer, GlobalWindow>() {
        @Override
        public void process(ProcessAllWindowFunction<Integer, Integer, GlobalWindow>.Context context, Iterable<Integer> elements, Collector<Integer> out) throws Exception {

        }
    });
    environment.execute();
}
```

> ä½¿ç”¨å…¨çª—å£æ‰§è¡Œå‡½æ•°,è¿›è¡Œèšåˆä¹‹åè°ƒç”¨å…¨çª—å£å‡½æ•°,æ‰€ä»¥å¯¼è‡´åªæœ‰ä¸€ä¸ªå…ƒç´ ,ä½†æ˜¯æ‹¿åˆ°äº†ä¸Šä¸‹æ–‡

### æ°´ä½çº¿

åœ¨æ•°æ®æµä¸­æœ‰ä¸¤ä¸ªæ—¶é—´æˆ³,åˆ†åˆ«æ˜¯äº‹ä»¶æ—¶é—´å’Œå¤„ç†æ—¶é—´,äº‹ä»¶æ—¶é—´æŒ‡çš„æ˜¯æ•°æ®çš„ç”Ÿäº§æ—¶é—´,
è€Œå¤„ç†æ—¶é—´æŒ‡çš„æ˜¯æ•°æ®çš„è¿›è¡Œå¤„ç†æ“ä½œçš„æ—¶é—´

æ°´ä½çº¿å’Œæ—¶é—´è¯­ä¹‰æ˜¯æ¯æ¯ç›¸å…³çš„,åœ¨çª—å£çš„å¤„ç†è¿‡ç¨‹ä¸­,æˆ‘ä»¬å¯ä»¥åŸºäºæ•°æ®çš„æ—¶é—´æˆ³,è‡ªå®šä¹‰ä¸€ä¸ªé€»è¾‘æ—¶é’Ÿ
é€»è¾‘æ—¶é’Ÿä¸ä¼šè·Ÿç€æ—¶é—´æ¨åŠ¨,ä»–æ˜¯æ ¹æ®æ•°æ®çš„æ—¶é—´è¿›è¡Œæ¨åŠ¨

åœ¨flinkä¸­ç”¨æ¥è¡¡é‡äº‹ä»¶æ—¶é—´çš„æ ‡è®°,å°±è¢«ç§°ä¸ºæ°´ä½çº¿(Watermark)

æ¯æ¡æ•°æ®å¹¶ä¸éƒ½ä¼šç”Ÿæˆæ°´ä½çº¿,è€Œæ˜¯æ¯å°æ®µæ—¶é—´ç”Ÿæˆæ°´ä½çº¿,
æœ‰åºçš„çŠ¶æ€ä¸‹å°±åƒä»¥ä¸‹è¿™æ ·å­çš„,æ°´ä½çº¿åœ¨æœ‰åºçš„æƒ…å†µä¸‹é€æ¸å¢å¤§

> => |17 15 |14 13 12 |11 10 9 8 |7 =>

åœ¨å®é™…çš„è¿‡ç¨‹ä¸­,æ•°æ®æ¥æ”¶å¯èƒ½ä¼šå‘ç”Ÿå»¶è¿Ÿ,å¯¼è‡´é¡ºåºæ¥æ”¶æ··ä¹±,è¿™å°±æ˜¯**ä¹±åºæ•°æ®**,ä¹±åºæ•°æ®çš„å¤„ç†æ–¹æ¡ˆå¾ˆç®€å•,
æ°´ä½çº¿åªä¼šå‰è¿›ä¸ä¼šåé€€,å°±åƒæ—¶é—´ä¸€æ ·,å‡è®¾ä»¥ä¸‹æ¯æ¡æ•°æ®éƒ½åŒ…å«æ°´ä½çº¿,åœ¨æ’å…¥æ°´ä½çº¿ä¸­åˆ¤æ–­æ˜¯å¦æ¯”å‰é¢çš„æ°´ä½çº¿å¤§,
å¦‚æœå¤§å†æ’å…¥,å¦è€…å°±åœæ­¢æ’å…¥æ°´ä½çº¿ç›´åˆ°æ¯”å½“å‰æ°´ä½çº¿å¤§çš„æ°´ä½çº¿å†æ’å…¥

> => 15 |17 13 |14 |12 10 |11 |9 |8 |7 =>

åœ¨ä¹±åºå’Œæ•°æ®é‡å¤§çš„æƒ…å†µä¸‹,ä¹Ÿå¯ä»¥ç”Ÿæˆæ°´ä½çº¿,åœ¨ç”Ÿæˆæ°´ä½çº¿ä¹‹å‰åˆ¤æ–­ä¼ è¿›æ¥çš„æ—¶é—´æˆ³
,æ‰¾åˆ°æœ€å¤§çš„æ—¶é—´æˆ³å¹¶ç”Ÿæˆæ°´ä½çº¿

> => |=17 15 17 |=14 13 14 12 |=11 11 9 =>

**è¿Ÿåˆ°çš„æ•°æ®**åŠ ä¹±åºä¹Ÿå¯ä»¥å¤„ç†,ä¸ºäº†è®©çª—å£èƒ½å¤Ÿæ”¶é›†åˆ°æ­£ç¡®çš„æ•°æ®,æˆ‘ä»¬ä¹Ÿå¯ä»¥ç­‰å¾…ä¸€æ®µæ—¶é—´ç”¨æ¥æ”¶é›†è¿Ÿåˆ°çš„æ•°æ®,
æ¯”å¦‚å‡å»ä¸¤ç§’,ç­‰å¾…ä¸¤ç§’åå†è¿›è¡Œå¤„ç†,å‡å¦‚æ•°æ®èµ°åˆ°äº†ä¹ç§’,æ°´ä½çº¿æŸ¥æ‰¾çš„æœ€å¤§æ•°æ®ä¹Ÿæ˜¯ä¹ç§’,
é€šè¿‡å‡å»ä¸¤ç§’é‚£ä¹ˆæ°´ä½çº¿æŒ‡å‘çš„å°±æ˜¯ä¸ƒç§’,ç­‰å¾…ä¹ç§’å†…æ²¡æœ‰æ”¶é›†åˆ°çš„æ•°æ®ç­‰å¾…ä¸¤ç§’

> => |=(17-2) 15 17 |=14-2 13 14 12 |=(11-2) 11 9 =>

> æ³¨æ„æ¯ä¸ªçª—å£ä¼šæ‰¾åˆ°è‡ªå·±çš„æ•°æ®,å¹¶ä¸ä¼šå‡å»ä¸¤ç§’ä¹‹åå¤„ç†ä¸å±äºè¿™ä¸¤ç§’çš„æ•°æ®

åœ¨DataStreamAPIä¸­æœ‰ä¸€ä¸ªç”Ÿæˆæ°´ä½çº¿çš„æ–¹æ³•`assignTimestampsAndWatermarks()`

```java
public static void main(String[] args) throws Exception {
    StreamExecutionEnvironment environment = StreamExecutionEnvironment.getExecutionEnvironment();
    ArrayList<Integer> strings = new ArrayList<>();
    for (int i = 0; i < 10; i++) {
        strings.add(i);
    }
    DataStreamSource<Integer> streamSource = environment.fromCollection(strings);
    //å®šä¹‰æ°´ä½çº¿
    WatermarkStrategy<Integer> integerWatermarkStrategy = WatermarkStrategy
            //å•è°ƒé€’å¢æ°´ä½çº¿
            .<Integer>forMonotonousTimestamps()
            //åˆ›å»ºæ—¶é—´æˆ³åˆ†é…å™¨ï¼Œä»åˆ†é…å™¨ä¸­æå–æ•°æ®
            .withTimestampAssigner(new SerializableTimestampAssigner<Integer>() {
                @Override
                public long extractTimestamp(Integer element, long recordTimestamp) {
                    System.out.println("Element: " + element + ", Event Time: " + element);
                    //è¿”å›äº‹ä»¶æ—¶é—´
                    return element * 1000L;
                }
            });
    //é…ç½®æ°´ä½çº¿
    streamSource.assignTimestampsAndWatermarks(integerWatermarkStrategy)
            //ä½¿ç”¨æ°´ä½çº¿æ—¶å¿…é¡»è¦ä½¿ç”¨äº‹ä»¶æ—¶é—´çª—å£
            .windowAll(TumblingEventTimeWindows.of(Time.seconds(1)))
            .reduce(Integer::sum)
            .print();
    environment.execute();
}
```

ä¹±åºçš„æ°´ä½çº¿API

```java
public static void main(String[] args) throws Exception {
    StreamExecutionEnvironment environment = StreamExecutionEnvironment.getExecutionEnvironment();
    ArrayList<Integer> strings = new ArrayList<>();
    for (int i = 0; i < 10; i++) {
        strings.add(i);
    }
    DataStreamSource<Integer> streamSource = environment.fromCollection(strings);
    //è®¾ç½®ç­‰å¾…æ—¶é—´ä¸ºä¸¤ç§’
    WatermarkStrategy<Integer> integerWatermarkStrategy = WatermarkStrategy.<Integer>forBoundedOutOfOrderness(Duration.ofSeconds(2))
            .withTimestampAssigner(new SerializableTimestampAssigner<Integer>() {
                @Override
                public long extractTimestamp(Integer element, long recordTimestamp) {
                    return element * 1000L;
                }
            });
    streamSource.assignTimestampsAndWatermarks(integerWatermarkStrategy)
            .windowAll(TumblingEventTimeWindows.of(Time.seconds(5)))
            .reduce(Integer::sum)
            .print();
    environment.execute();
}
```